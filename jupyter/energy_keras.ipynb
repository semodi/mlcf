{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/smdick/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Add\n",
    "from keras import regularizers\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import read\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from ase import Atoms\n",
    "\n",
    "class Dataset():\n",
    "    \"\"\"\n",
    "    Dataset that saves the training and test data (split automatically)\n",
    "    along with the associated atomic species\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.atoms = None\n",
    "        self.features = {} \n",
    "        self.energies = np.zeros([0])\n",
    "        self.forces = None\n",
    "        self.X_train = {}\n",
    "        self.X_test = {}\n",
    "        self.y_train = 0\n",
    "        self.y_test = 0\n",
    "        \n",
    "    def import_features(self, file, species):\n",
    "        features = np.genfromtxt(file, delimiter = ',')\n",
    "        self.set_features(features)\n",
    "        \n",
    "    def import_atoms(self, file):\n",
    "        \"\"\"\n",
    "        Import processed features from either .traj, .xyz or .csv file.\n",
    "        If .csv file is imported additional information has\n",
    "        to be given in kwargs\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def import_energies(self, file):\n",
    "        pass\n",
    "    \n",
    "    def split(self, test_size=0.2, seed = 42):\n",
    "        \n",
    "        for key in self.features:\n",
    "            \n",
    "            self.X_train[key], self.X_test[key], self.y_train, self.y_test =\\\n",
    "                train_test_split(self.features[key].reshape(len(self.energies),-1),self.energies, \n",
    "                                test_size=0.2, random_state=42)\n",
    "                \n",
    "            self.X_train[key] = self.X_train[key].reshape(-1,self.features[key].shape[1])\n",
    "            self.X_test[key] = self.X_test[key].reshape(-1,self.features[key].shape[1])\n",
    "            \n",
    "    def set_features(self, features, species):\n",
    "        if species in self.features:\n",
    "            if self.features[species].shape[1] == features.shape[1]:\n",
    "                self.features[species] = np.concatenate([self.features[species], features])\n",
    "            else:\n",
    "                raise Exception('Shape of new features {} inconsistent'.format(features.shape) +\\\n",
    "                                'with features already imported {}'.format(self.features[species].shape))\n",
    "        else:\n",
    "            self.features[species] = features\n",
    "    \n",
    "    def set_atoms(self, descr_str):\n",
    "        self.atoms = Atoms(descr_str)\n",
    "    \n",
    "    def set_energies(self, energies):\n",
    "        energies = np.array(energies)\n",
    "        if energies.ndim != 1 and energies.shape[1] != 1:\n",
    "            raise Exception('Please provide 1-d array for energies')\n",
    "        self.energies = np.concatenate([self.energies, energies.flatten()])\n",
    "      \n",
    "    def get_atoms(self):\n",
    "        return self.atoms\n",
    "    \n",
    "    def get_features(self, species, which='train'):\n",
    "        if which=='train':\n",
    "            return self.X_train[species]\n",
    "        elif which=='test':\n",
    "            return self.X_test[species]\n",
    "        elif which=='all':\n",
    "            return self.features[species]\n",
    "        else:\n",
    "            raise Error('which = '+ which + ' is an invalid option')\n",
    "            \n",
    "    def get_energies(self, which = 'train'):\n",
    "        if which=='train':\n",
    "            return self.y_train\n",
    "        elif which=='test':\n",
    "            return self.y_test\n",
    "        elif which=='all':\n",
    "            return self.energies\n",
    "        else:\n",
    "            raise Error('which = '+ which + ' is an invalid option')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Add\n",
    "from keras import regularizers\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_atoms_per_species(dataset):\n",
    "    \"\"\" \n",
    "    Determines how many atom of each species are contained in \n",
    "    one sample of a homogeneous dataset\n",
    "    \"\"\"\n",
    "    species = get_unique_species([dataset])\n",
    "    n_per_species = {}\n",
    "    \n",
    "    for s in species:\n",
    "        n_per_species[s] = np.sum(np.array(dataset.get_atoms().get_chemical_symbols()) == s)\n",
    "    return n_per_species    \n",
    "    \n",
    "def get_unique_species(datasets):\n",
    "    \"\"\" \n",
    "    Determines how many unique atomic species are contained in \n",
    "    a collection of datasets\n",
    "    \"\"\"\n",
    "    symbols = [] \n",
    "    for dataset in datasets:\n",
    "        symbols.append(np.unique(dataset.get_atoms().get_chemical_symbols()))\n",
    "    symbols = np.unique(np.concatenate(symbols))\n",
    "\n",
    "    return symbols\n",
    "\n",
    "\n",
    "class BatchGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, inputs, outputs, batch_size):\n",
    "        if not len(inputs) == len(outputs):\n",
    "            raise Exception('Inputs and outputs list must have same length',\n",
    "                            'maybe group inputs by datasets')\n",
    "        \n",
    "        self.iteration = 0\n",
    "        self.all_inputs, self.all_outputs = inputs, outputs\n",
    "        self.smallest_size = min([len(out) for out in self.all_outputs])\n",
    "        self.batch_size = batch_size\n",
    "        self.update_in_out()\n",
    "        self.length = min([int(np.ceil(len(out)/batch_size)) for out in self.outputs])\n",
    "        \n",
    "    def update_in_out(self):\n",
    "        \n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        for ds_id, inp_dataset in enumerate(self.all_inputs):\n",
    "            self.inputs.append([])\n",
    "            for inp_atomic in inp_dataset:\n",
    "                self.inputs[ds_id].append(\\\n",
    "                        np.take(inp_atomic, np.arange(self.iteration*self.smallest_size,\n",
    "                                   (self.iteration+1)*self.smallest_size), axis =0, mode='wrap'))\n",
    "        \n",
    "        for ds_id, out_dataset in enumerate(self.all_outputs):\n",
    "            self.outputs.append(\\\n",
    "                    np.take(out_dataset, np.arange(self.iteration*self.smallest_size,\n",
    "                                   (self.iteration+1)*self.smallest_size), axis =0, mode='wrap'))\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_in = []\n",
    "        batch_out = []\n",
    "        for inp_dataset in self.inputs:\n",
    "            for inp_atomic in inp_dataset:\n",
    "                batch_in.append(inp_atomic[idx * self.batch_size:(idx + 1) * self.batch_size])\n",
    "       \n",
    "        for out_dataset in self.outputs:\n",
    "            batch_out.append(out_dataset[idx * self.batch_size:(idx + 1) * self.batch_size])\n",
    "\n",
    "        if idx == self.length - 1:\n",
    "            min_length = min([len(out) for out in batch_out])\n",
    "            for i,_ in enumerate(batch_in):\n",
    "                batch_in[i] = batch_in[i][:min_length]\n",
    "            for i,_ in enumerate(batch_out):\n",
    "                batch_out[i] = batch_out[i][:min_length]    \n",
    "        return batch_in, batch_out\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        self.iteration += 1\n",
    "        self.update_in_out()\n",
    "#         print(np.array(self.inputs).shape)\n",
    "#         print(np.array(self.outputs).shape)\n",
    "        for i, (inp_dataset, out_dataset) in enumerate(zip(self.inputs, self.outputs)):\n",
    "            *self.inputs[i], self.outputs[i] = shuffle(*inp_dataset, out_dataset)\n",
    "\n",
    "class Functional():\n",
    "    \n",
    "    def __init__(self, datasets):\n",
    "        pass\n",
    "            \n",
    "    def fit(self, *args,**kwargs):\n",
    "        pass\n",
    "    \n",
    "   \n",
    "        \n",
    "class EnergyFunctional(Functional):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.defaults = {'layers': 3, 'nodes': 16, 'activations': 'sigmoid','reg' : 0}\n",
    "        self.target_shape = 1\n",
    "        self.hidden_layers = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "        # The model used to fit the datasets\n",
    "        self.model = None\n",
    "        \n",
    "        # (Sub-)Models that are used to treat single atoms of a given species\n",
    "        self.atomic_models = {}\n",
    "        \n",
    "        \n",
    "    def build_submodel(self, species, input_shape):\n",
    "        \n",
    "        inputs = Input(shape = (input_shape,))\n",
    "        x = inputs\n",
    "        for hlayer in self.hidden_layers[species]:\n",
    "            x = hlayer(x)\n",
    "        outputs = x\n",
    "    \n",
    "        return inputs, outputs    \n",
    "    \n",
    "    def fit_scalers(self, datasets):\n",
    "        \n",
    "        for species in self.species:\n",
    "            all_feat = []\n",
    "            for dataset in datasets:\n",
    "                all_feat.append(dataset.get_features(species, which = 'all'))\n",
    "            all_feat = np.concatenate(all_feat)\n",
    "        \n",
    "            self.scalers[species] = MinMaxScaler()\n",
    "            self.scalers[species].fit(all_feat)\n",
    "        \n",
    "                \n",
    "    def build_from_datasets(self, datasets):\n",
    "        \n",
    "        # Identify unique atomic species\n",
    "        if not isinstance(datasets, list): \n",
    "            datasets = [datasets]\n",
    "            \n",
    "        self.datasets = datasets\n",
    "        self.species = get_unique_species(datasets)\n",
    "        self.fit_scalers(datasets)\n",
    "        for dataset in datasets:\n",
    "            dataset.split()\n",
    "            \n",
    "        # Build hidden layers for each species\n",
    "        for s in self.species:\n",
    "            self.hidden_layers[s] = []\n",
    "            for layer in range(self.defaults['layers']):\n",
    "                self.hidden_layers[s].append(Dense(units=self.defaults['nodes'],\n",
    "                                           activation=self.defaults['activations'],\n",
    "                                           kernel_regularizer=regularizers.l2(self.defaults['reg'])))\n",
    "            \n",
    "            self.hidden_layers[s].append(Dense(units=self.target_shape,\n",
    "                                       activation='linear',\n",
    "                                       kernel_regularizer=regularizers.l2(self.defaults['reg'])))\n",
    "                \n",
    "        # Build model\n",
    "        all_inputs = []\n",
    "        all_outputs = []\n",
    "        input_shape_species = {}\n",
    "        for dataset in datasets:\n",
    "            inputs = []\n",
    "            outputs = []\n",
    "            \n",
    "            atoms_per_species = get_atoms_per_species(dataset)\n",
    "            unique_species = get_unique_species([dataset]) \n",
    "            \n",
    "            for species in unique_species:\n",
    "                input_shape = dataset.get_features(species).shape[1]\n",
    "                input_shape_species[species] = input_shape\n",
    "#                 print(atoms_per_species[species])\n",
    "                for input_n in range(atoms_per_species[species]):\n",
    "                    in_t, out_t = self.build_submodel(species, input_shape)\n",
    "                    inputs.append(in_t)\n",
    "                    outputs.append(out_t)\n",
    "                \n",
    "            output = Add()(outputs)\n",
    "            all_outputs.append(output)\n",
    "            all_inputs += inputs\n",
    "       \n",
    "        self.model = Model(inputs=all_inputs, outputs = all_outputs) \n",
    "        \n",
    "        # Build atomic models\n",
    "        for species in self.species:\n",
    "            in_atomic, out_atomic = self.build_submodel(species, input_shape_species[species])\n",
    "            self.atomic_models[species] = Model(inputs=in_atomic, outputs = out_atomic)\n",
    "    \n",
    "    def get_in_out(self):\n",
    "        \n",
    "        all_inputs = []\n",
    "        all_outputs = []\n",
    "        \n",
    "        for dataset in self.datasets:\n",
    "            atoms_per_species = get_atoms_per_species(dataset)\n",
    "            unique_species = get_unique_species([dataset]) \n",
    "            inputs = []\n",
    "            for species in unique_species:\n",
    "                for n in range(atoms_per_species[species]):\n",
    "                    inputs.append(self.scalers[species].transform(\\\n",
    "                            dataset.get_features(species)[n::atoms_per_species[species]]))\n",
    "                    \n",
    "            all_inputs.append(inputs)\n",
    "            all_outputs.append(dataset.get_energies())\n",
    "        return all_inputs, all_outputs\n",
    "    \n",
    "    def fit(self, epochs=10, batch_size=32):\n",
    "        inputs, outputs = self.get_in_out()\n",
    "        seq = BatchGenerator(inputs, outputs, batch_size)\n",
    "        self.model.fit_generator(seq, epochs=epochs, shuffle = False, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, n_mol, n_o_orb=18, n_h_orb=8):\n",
    "    feat = np.genfromtxt(path + 'descriptors_coeff.dat', delimiter = ',')\n",
    "    tar  = np.genfromtxt(path + 'energies.dat', delimiter = ',') -\\\n",
    "            np.genfromtxt(path + 'energies_siesta.dat', delimiter = ',') - 469.766523 * n_mol\n",
    "    dataset = Dataset()\n",
    "    dataset.set_atoms('{}OHH'.format(n_mol))\n",
    "    feat = feat.reshape(len(feat)*n_mol, -1)\n",
    "    dataset.set_features(feat[:,:n_o_orb], 'O')\n",
    "    dataset.set_features(feat[:,n_o_orb:].reshape(-1,n_h_orb), 'H')\n",
    "    dataset.set_energies(tar)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '/gpfs/home/smdick/exchange_ml/data/synced/processed/'\n",
    "monomers = get_dataset(basepath + 'monomers/final_small/', 1)\n",
    "dimers = get_dataset(basepath + 'dimers/final/', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = EnergyFunctional()\n",
    "func.build_from_datasets([monomers, dimers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.model.compile(loss='mean_squared_error',\n",
    "              optimizer=keras.optimizers.Adam(0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 8.0745e-04 - add_25_loss: 8.8975e-05 - add_26_loss: 7.1847e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0170 - add_25_loss: 0.0019 - add_26_loss: 0.0151 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00        \n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0032 - add_25_loss: 5.4987e-04 - add_26_loss: 0.0026 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0010 - add_25_loss: 1.0800e-04 - add_26_loss: 9.2766e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.5858e-04 - add_25_loss: 3.7500e-05 - add_26_loss: 5.2108e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 9.2085e-04 - add_25_loss: 5.9623e-05 - add_26_loss: 8.6122e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0013 - add_25_loss: 1.2800e-04 - add_26_loss: 0.0012 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 7.8727e-04 - add_25_loss: 7.5112e-05 - add_26_loss: 7.1216e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0138 - add_25_loss: 0.0012 - add_26_loss: 0.0126 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00        \n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0106 - add_25_loss: 0.0020 - add_26_loss: 0.0086 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0031 - add_25_loss: 5.1740e-04 - add_26_loss: 0.0026 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 9.4260e-04 - add_25_loss: 9.8673e-05 - add_26_loss: 8.4393e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0015 - add_25_loss: 1.3714e-04 - add_26_loss: 0.0013 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 5.7811e-04 - add_25_loss: 6.3958e-05 - add_26_loss: 5.1416e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0105 - add_25_loss: 5.5856e-04 - add_26_loss: 0.0100 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0037 - add_25_loss: 6.5046e-04 - add_26_loss: 0.0031 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0014 - add_25_loss: 2.0569e-04 - add_26_loss: 0.0012 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0012 - add_25_loss: 1.7295e-04 - add_26_loss: 0.0010 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0015 - add_25_loss: 1.9282e-04 - add_26_loss: 0.0013 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0016 - add_25_loss: 1.9597e-04 - add_26_loss: 0.0014 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0012 - add_25_loss: 1.6577e-04 - add_26_loss: 9.9076e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0101 - add_25_loss: 4.9140e-04 - add_26_loss: 0.0096 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0064 - add_25_loss: 0.0012 - add_26_loss: 0.0052 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 8.0136e-04 - add_25_loss: 1.0225e-04 - add_26_loss: 6.9910e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.0004e-04 - add_25_loss: 4.4060e-05 - add_26_loss: 4.5598e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0013 - add_25_loss: 1.5578e-04 - add_26_loss: 0.0011 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0015 - add_25_loss: 1.7837e-04 - add_26_loss: 0.0013 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0010 - add_25_loss: 1.4258e-04 - add_26_loss: 8.6639e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0149 - add_25_loss: 0.0016 - add_26_loss: 0.0134 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00        \n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0033 - add_25_loss: 6.1664e-04 - add_26_loss: 0.0027 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0013 - add_25_loss: 2.0134e-04 - add_26_loss: 0.0011 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0011 - add_25_loss: 1.5733e-04 - add_26_loss: 9.8857e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0019 - add_25_loss: 2.3259e-04 - add_26_loss: 0.0016 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0030 - add_25_loss: 5.7970e-04 - add_26_loss: 0.0024 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0143 - add_25_loss: 0.0013 - add_26_loss: 0.0130 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0108 - add_25_loss: 0.0021 - add_26_loss: 0.0087 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0021 - add_25_loss: 3.5073e-04 - add_26_loss: 0.0018 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 8.6166e-04 - add_25_loss: 1.1171e-04 - add_26_loss: 7.4996e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0013 - add_25_loss: 1.6043e-04 - add_26_loss: 0.0012 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 7.6982e-04 - add_25_loss: 4.4786e-05 - add_26_loss: 7.2504e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 5.4945e-04 - add_25_loss: 6.1270e-05 - add_26_loss: 4.8818e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0122 - add_25_loss: 9.6620e-04 - add_26_loss: 0.0112 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0044 - add_25_loss: 7.8883e-04 - add_26_loss: 0.0036 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0012 - add_25_loss: 1.6711e-04 - add_26_loss: 0.0010 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 5.5481e-04 - add_25_loss: 6.7204e-05 - add_26_loss: 4.8761e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 8ms/step - loss: 6.4165e-04 - add_25_loss: 5.8421e-05 - add_26_loss: 5.8323e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0011 - add_25_loss: 1.1588e-04 - add_26_loss: 0.0010 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.7178e-04 - add_25_loss: 5.5334e-05 - add_26_loss: 5.1645e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0146 - add_25_loss: 0.0015 - add_26_loss: 0.0131 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00        \n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0033 - add_25_loss: 6.1298e-04 - add_26_loss: 0.0027 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 8.7934e-04 - add_25_loss: 1.2947e-04 - add_26_loss: 7.4987e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 7.0135e-04 - add_25_loss: 6.2492e-05 - add_26_loss: 6.3886e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 9.2103e-04 - add_25_loss: 8.2866e-05 - add_26_loss: 8.3816e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0012 - add_25_loss: 2.0047e-04 - add_26_loss: 9.9742e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0183 - add_25_loss: 0.0020 - add_26_loss: 0.0163 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0059 - add_25_loss: 0.0011 - add_26_loss: 0.0048 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00        \n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0012 - add_25_loss: 2.1216e-04 - add_26_loss: 0.0010 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.6909e-04 - add_25_loss: 6.3244e-05 - add_26_loss: 5.0585e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 5.5495e-04 - add_25_loss: 4.2448e-05 - add_26_loss: 5.1250e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.6712e-04 - add_25_loss: 4.6758e-05 - add_26_loss: 6.2036e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 5.4828e-04 - add_25_loss: 6.4969e-05 - add_26_loss: 4.8331e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0110 - add_25_loss: 7.0928e-04 - add_26_loss: 0.0103 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - add_25_loss: 5.7465e-04 - add_26_loss: 0.0026 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 9.9547e-04 - add_25_loss: 1.3775e-04 - add_26_loss: 8.5772e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.3267e-04 - add_25_loss: 8.8440e-05 - add_26_loss: 5.4423e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 7.6967e-04 - add_25_loss: 8.0129e-05 - add_26_loss: 6.8954e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.2584e-04 - add_25_loss: 4.5673e-05 - add_26_loss: 5.8017e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.9484e-04 - add_25_loss: 5.2448e-05 - add_26_loss: 4.4240e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0118 - add_25_loss: 8.5147e-04 - add_26_loss: 0.0110 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0080 - add_25_loss: 0.0017 - add_26_loss: 0.0064 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0026 - add_25_loss: 4.6049e-04 - add_26_loss: 0.0022 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0013 - add_25_loss: 2.1272e-04 - add_26_loss: 0.0011 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0012 - add_25_loss: 1.2058e-04 - add_26_loss: 0.0010 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 8.0546e-04 - add_25_loss: 1.2270e-04 - add_26_loss: 6.8276e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0154 - add_25_loss: 0.0016 - add_26_loss: 0.0138 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0030 - add_25_loss: 5.5093e-04 - add_26_loss: 0.0025 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0011 - add_25_loss: 1.8187e-04 - add_26_loss: 8.9016e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0012 - add_25_loss: 1.9202e-04 - add_26_loss: 0.0010 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0023 - add_25_loss: 4.1080e-04 - add_26_loss: 0.0019 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0020 - add_25_loss: 2.8284e-04 - add_26_loss: 0.0018 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 9.3120e-04 - add_25_loss: 1.2813e-04 - add_26_loss: 8.0307e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0115 - add_25_loss: 7.7007e-04 - add_26_loss: 0.0108 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0023 - add_25_loss: 3.9364e-04 - add_26_loss: 0.0019 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 7.9300e-04 - add_25_loss: 1.1435e-04 - add_26_loss: 6.7865e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 7.2734e-04 - add_25_loss: 9.1394e-05 - add_26_loss: 6.3594e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 8.8636e-04 - add_25_loss: 8.2542e-05 - add_26_loss: 8.0381e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0011 - add_25_loss: 1.3706e-04 - add_26_loss: 0.0010 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.7538e-04 - add_25_loss: 8.9311e-05 - add_26_loss: 5.8607e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0164 - add_25_loss: 0.0019 - add_26_loss: 0.0146 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00        \n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0053 - add_25_loss: 0.0010 - add_26_loss: 0.0043 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0022 - add_25_loss: 4.3661e-04 - add_26_loss: 0.0018 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0014 - add_25_loss: 2.1295e-04 - add_26_loss: 0.0012 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 6.4289e-04 - add_25_loss: 3.1977e-05 - add_26_loss: 6.1092e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 6.7451e-04 - add_25_loss: 1.1761e-04 - add_26_loss: 5.5691e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0155 - add_25_loss: 0.0016 - add_26_loss: 0.0139 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00        \n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0031 - add_25_loss: 5.3066e-04 - add_26_loss: 0.0025 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 9.3606e-04 - add_25_loss: 1.3565e-04 - add_26_loss: 8.0041e-04 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0016 - add_25_loss: 2.6523e-04 - add_26_loss: 0.0013 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0017 - add_25_loss: 2.6155e-04 - add_26_loss: 0.0015 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00    \n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0011 - add_25_loss: 1.2545e-04 - add_26_loss: 0.0010 - add_25_acc: 0.0000e+00 - add_26_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "func.fit(epochs = 100, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aabc69c6978>]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aabc4142320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(func.model.predict([feat_h1,feat_h2,feat_o]), tar, ls = '', marker = '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_o = func.atomic_models['O']\n",
    "model_h = func.atomic_models['H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(func.model.predict([feat_h1,feat_h2,feat_o]),\n",
    "            model_o.predict(feat_o) + model_h.predict(feat_h1) + model_h.predict(feat_h2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
